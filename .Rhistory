from sklearn.pipeline import Pipeline
# Read Data
X_train = r.X_train_py
X_test = r.X_test_py
y_train = r.y_train_py
y_test = r.y_test_py
# The KFold object in SciKit Learn tells the cross validation object (see below) how to split up the data:
# Initiate KFold Object
kf = KFold(shuffle=True, random_state=777777, n_splits=4)
kf.split(X_train) # is a generator object
X_train.describe()
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import GridSearchCV
# Same estimator as before:
estimator = Pipeline([("scaler", StandardScaler()),
("ridge_regression", Ridge())])
params = {
'ridge_regression__alpha': np.geomspace(4, 20, 30),
}
grid = GridSearchCV(estimator, params, cv=kf)
grid.fit(X_train, y_train)
grid.best_score_, grid.best_params_
y_predict_test = grid.predict(X_test)
y_predict_train = grid.predict(X_train)
r2_score(y_train, y_predict_train)
# This includes both in-sample and out-of-sample
r2_score(y_test, y_predict_test)
# Notice that "grid" is a fit object!
# We can use grid.predict(X_test) to get brand new predictions!
Ridge_Coefs = grid.best_estimator_.named_steps['ridge_regression'].coef_
Varnames = X_train.columns
# Zip the two lists together
zipped_list = list(zip(Varnames, Ridge_Coefs))
# Convert the zipped list to a DataFrame
df = pd.DataFrame(zipped_list, columns=['Variable', 'Coefficient'])
# Print the DataFrame
df.sort_values(by = 'Coefficient', ascending = False)
print(df)
df.to_csv('Ridge_Coefficients.csv')
GridResults = pd.DataFrame(grid.cv_results_)
GridResults.rank_test_score.sort_values()
print(GridResults.loc[29])
print(GridResults.params.loc[29])
grid.best_estimator_
df<-read.csv("Baseline_Calendly_Hubspot Data Combined [no emails].csv", header=T, sep=",", encoding='utf-8-rom')
# Data Load and Missingness part 1 ----------------------------------------
# Examine patterns of missing data
library(finalfit)
finalfit::missing_plot(df)
# View Columns in Dataset
colnames(df)
# Turn NA for Graduate Student (0 vs. 1) into 0's
df$GraduateStudent<-ifelse(is.na(df$GraduateStudent)==T, 0, df$GraduateStudent)
# Create a new dataframe to display missing data for each variable
missing_df<-as.data.frame(matrix(data=NA, ncol = 3, nrow = ncol(df)))
colnames(missing_df)<-c("variable", "missingcount", "missingproportion")
missing_df$variable<-colnames(df)
for(i in 1:ncol(df)){
missing_df[i,2]<-sum(is.na(df[,i])==T)
missing_df[i,3]<-sum(is.na(df[,i])==T) / nrow(df)
}
# Compile list of variables that are missing 100% of rows...
missing_all<-missing_df[missing_df$missingproportion==1,]
allmissing_list<-missing_all$variable
# Print the list of variables that are missing at 100%
allmissing_list
# Make new dataframe that removes all variables that have 100% missingness
df_missing_removed<-dplyr::select(df, -allmissing_list)
colnames(df_missing_removed)
# Removing more columns that cannot be used (i.e., TEXT responses to some 'other' items)
df_missing_removed<- df_missing_removed[,-c(1:8, 10:20, 23:26, 15,16,26,36,52,53,56,58,68:69,71:80,90:95,115,121:287,324:334,349:410,420:424)]
finalfit::missing_plot(df_missing_removed)
colnames(df_missing_removed)
# Find column numbers for variables that need to be converted to "numeric"
# Also turns NAs in these columns into 0's
numcols<-c(1,3:28,32:39, 42:55, 68:70, 73:109)
for(col in numcols){
df_missing_removed[,col]<-as.numeric(unlist(df_missing_removed[,col]))
df_missing_removed[,col]<-ifelse(is.na(df_missing_removed[,col])==T, 0, df_missing_removed[,col])
}
finalfit::missing_plot(df_missing_removed)
# Create a new dataframe to display missing data for each variable
missing_df<-as.data.frame(matrix(data=NA, ncol = 3, nrow = ncol(df_missing_removed)))
colnames(missing_df)<-c("variable", "missingcount", "missingproportion")
missing_df$variable<-colnames(df_missing_removed)
for(i in 1:ncol(df_missing_removed)){
missing_df[i,2]<-sum(is.na(df_missing_removed[,i])==T)
missing_df[i,3]<-sum(is.na(df_missing_removed[,i])==T) / nrow(df_missing_removed)
}
# Remove last bits of high missingness in dataframe
colnames(df_missing_removed)
df_missing_removed<-dplyr::select(df_missing_removed, -B_Transgender, -S_LangChoice)
finalfit::missing_plot(df_missing_removed)
# Remove NA cases from final dataframe (clean_df)
clean_df<-na.omit(df_missing_removed)
finalfit::missing_plot(clean_df)
colnames(clean_df)
# Recorded Date and Student Stress Level ----------------------------------
# First 2 weeks of semester is very low stress - syllabi weeks, no quizzes and minimal homework
# Stress increases around the first month for most students probably
# Stress peaks for most student probably reading week before finals and before grades come back
clean_df$RecordedDate
# Critical Dates for stress in the semester:
# Lowest Stress (0) = 1/17/24 - 2/14/24
# Coasting Stress (1) = 2/15/24 - 3/15/24
# Lower Stress (0) = 3/16/24 - 3/22/24
# Highest Stress (1) = 3/23/24 - 5/8/24
## CHAT GPT PROMPT ##
# I have a column called RecordedDate in an R dataframe called clean_df. Each row of RecordedDate is formatted like this: 2/20/2024 10:51
# Can you write me some R code using the lubridate package that will create a new variable called SemesterStress
# Use the values of RecordedDate to assign SemesterStress a 0 or 1 according to the guidelines below:
# SemesterStress = 0 if RecordedDate is 1/17/24 - 2/14/24 or 3/16/24 - 3/22/24
# SemesterStress = 1 if RecordedDate is 2/15/24 - 3/15/24 or 3/23/24 - 5/8/24
# Load the necessary library
library(lubridate)
# Convert RecordedDate to datetime format
clean_df$RecordedDate <- mdy_hm(clean_df$RecordedDate)
# Create the SemesterStress variable
clean_df$SemesterStress <- with(clean_df, ifelse(
(RecordedDate >= mdy("1/17/2024") & RecordedDate <= mdy("2/14/2024")) |
(RecordedDate >= mdy("3/16/2024") & RecordedDate <= mdy("3/22/2024")),
0,
ifelse(
(RecordedDate >= mdy("2/15/2024") & RecordedDate <= mdy("3/15/2024")) |
(RecordedDate >= mdy("3/23/2024") & RecordedDate <= mdy("5/8/2024")),
1,
NA  # Assign NA if the date does not fall into any of the specified ranges
)
))
table(clean_df$SemesterStress)
hist(clean_df$SemesterStress)
# Display the dataframe with the new SemesterStress variable
print(clean_df)
head(clean_df)
# Prepare data for investigating depression
clean_df<-clean_df[,-c(2)]
colnames(clean_df)
clean_df<-clean_df[clean_df$Hispanic < 94, ]
clean_df<-clean_df[clean_df$UA_SUICIDE_Attempted < 5, ]
clean_df<-na.omit(clean_df)
finalfit::missing_plot(clean_df)
clean_df$SemesterStress<-as.numeric(unlist(clean_df$SemesterStress))
clean_df$UA_SUICIDE_Considered
table(clean_df$UA_SUICIDE_Considered)
clean_df<-clean_df[clean_df$UA_SUICIDE_Considered < 5, ]
table(clean_df$UA_SUICIDE_Considered)
clean_df$AttendedAppt
table(clean_df$AttendedAppt)
clean_df$
library(caTools)
clean_df$Diener1
table(clean_df$Diener1)
library(caTools)
set.seed(123)  # For reproducibility
split <- sample.split(clean_df$UA_SUICIDE_Attempted, SplitRatio = 0.7)  # 70% training data
# Create training and testing sets
training <- subset(clean_df, split == TRUE)
testing <- subset(clean_df, split == FALSE)
colnames(clean_df)
colnames(clean_df)
colnames(clean_df)
colnames(clean_df)
MH_data<- clean_df[,71:105]
MH_data<- clean_df[,71:105]
View(MH_data)
colnames(MH_data)<-MHcol_names
MHcol_names<-c("Brief psychotic disorder ","Unspecified psychosis ","Bipolar disorder ","Major depressive disorder",
"Major depressive disorder","Manic episode ","Persistent mood (affective) disorders ",
"Unspecified mood (affective) disorders ","Agoraphobia without panic disorder ",
"Agoraphobia with panic disorder ","Agoraphobia","Generalized anxiety disorder ","Panic disorder ",
"Phobic anxiety disorders ","Social phobias (social anxiety disorder) ",
"Specific (isolated) phobias ","Excoriation (skin-picking) disorder ","Obsessive-compulsive disorder ",
"Obsessive-compulsive disorder with mixed obsessional thoughts and acts ",
"Acute stress disorder; reaction to severe stress","Adjustment disorders ",
"Body dysmorphic disorder ","Dissociative and conversion disorders ","Post traumatic stress disorder ",
"Eating disorders ","Sleep disorders not due to a substance or known physiological condition ",
"Antisocial personality disorder ","Avoidant personality disorder ","Borderline personality disorder ",
"Intellectual disabilities ","Obsessive compulsie personality disorder ",
"Other specific personality disorder ","Paranoid personality disorder ",
"Personality disorder","Pervasive and specific developmental disorders ")
MH_data<- clean_df[,71:105]
colnames(MH_data)<-MHcol_names
clean_df[,71:105]<-MH_data
colnames(clean_df)
MHcol_names<- colnames(clean_df[,71:105])
colnames(clean_df)
MHcol_names<-c("Brief psychotic disorder ","Unspecified psychosis ","Bipolar disorder ","Major depressive disorder",
"Major depressive disorder","Manic episode ","Persistent mood (affective) disorders ",
"Unspecified mood (affective) disorders ","Agoraphobia without panic disorder ",
"Agoraphobia with panic disorder ","Agoraphobia","Generalized anxiety disorder ","Panic disorder ",
"Phobic anxiety disorders ","Social phobias (social anxiety disorder) ",
"Specific (isolated) phobias ","Excoriation (skin-picking) disorder ","Obsessive-compulsive disorder ",
"Obsessive-compulsive disorder with mixed obsessional thoughts and acts ",
"Acute stress disorder; reaction to severe stress","Adjustment disorders ",
"Body dysmorphic disorder ","Dissociative and conversion disorders ","Post traumatic stress disorder ",
"Eating disorders ","Sleep disorders not due to a substance or known physiological condition ",
"Antisocial personality disorder ","Avoidant personality disorder ","Borderline personality disorder ",
"Intellectual disabilities ","Obsessive compulsie personality disorder ",
"Other specific personality disorder ","Paranoid personality disorder ",
"Personality disorder","Pervasive and specific developmental disorders ")
colnames(clean_df[,71:105])<-MHcol_names
colnames(clean_df)
CleanColNames<-colnames(clean_df)
CleanColNames<-colnames(clean_df)
write.csv(CleanColNames, "CleanColNames.csv", row.names = F)
clean_cols<-read.csv("CleanColNames.csv", header = T, sep=",", encoding = 'utf-8-rom')
colnames(clean_df)<-clean_cols$x
colnames(clean_df)
library(caTools)
set.seed(123)  # For reproducibility
split <- sample.split(clean_df$UA_SUICIDE_Attempted, SplitRatio = 0.7)  # 70% training data
# Create training and testing sets
training <- subset(clean_df, split == TRUE)
testing <- subset(clean_df, split == FALSE)
X_train<-training[,-c(120)]
X_test<-testing[,-c(120)]
y_train<-training[,c(120)]
y_test<-testing[,c(120)]
colnames(X_train)
library(reticulate)
use_condaenv("mlbase", required=T)
py_config()
# Convert R objects to PY objects (X_train_py is the python object --------
X_train_py<-r_to_py(X_train)
X_test_py<-r_to_py(X_test)
y_train_py<-r_to_py(y_train)
y_test_py<-r_to_py(y_test)
reticulate::repl_python()
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import r2_score
from sklearn.pipeline import Pipeline
# Read Data
X_train = r.X_train_py
X_test = r.X_test_py
y_train = r.y_train_py
y_test = r.y_test_py
kf = KFold(shuffle=True, random_state=777777, n_splits=4)
kf.split(X_train) # is a generator object
X_train.describe()
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import GridSearchCV
# Same estimator as before:
estimator = Pipeline([("scaler", StandardScaler()),
("ridge_regression", Ridge())])
params = {
'ridge_regression__alpha': np.geomspace(4, 20, 30),
}
grid = GridSearchCV(estimator, params, cv=kf)
grid.fit(X_train, y_train)
grid.best_score_, grid.best_params_
y_predict_test = grid.predict(X_test)
y_predict_train = grid.predict(X_train)
r2_score(y_train, y_predict_train)
# This includes both in-sample and out-of-sample
r2_score(y_test, y_predict_test)
# Notice that "grid" is a fit object!
# We can use grid.predict(X_test) to get brand new predictions!
Ridge_Coefs = grid.best_estimator_.named_steps['ridge_regression'].coef_
Varnames = X_train.columns
# Zip the two lists together
zipped_list = list(zip(Varnames, Ridge_Coefs))
# Convert the zipped list to a DataFrame
df = pd.DataFrame(zipped_list, columns=['Variable', 'Coefficient'])
# Print the DataFrame
df.sort_values(by = 'Coefficient', ascending = False)
print(df)
df.to_csv('Ridge_Coefficients.csv')
GridResults = pd.DataFrame(grid.cv_results_)
GridResults.rank_test_score.sort_values()
print(GridResults.loc[29])
print(GridResults.params.loc[29])
grid.best_estimator_
quit
ridge_coef<- read.csv("Ridge_Coefficients.csv", header = T, sep = ',', encoding = 'utf-8-rom')
ridge_coef$Abs_value <- abs(ridge_coef$Coefficient)
View(ridge_coef)
ridge_coef<- ridge_coef[ridge_coef$Abs_value > .03, ]
New_df<-dplyr::select(clean_df, UA_SUICIDE_Attempted, ridge_coef$Variable)
df<-read.csv("Baseline_Calendly_Hubspot Data Combined [no emails].csv", header=T, sep=",", encoding='utf-8-rom')
# Data Load and Missingness part 1 ----------------------------------------
# Examine patterns of missing data
library(finalfit)
finalfit::missing_plot(df)
# View Columns in Dataset
colnames(df)
# Turn NA for Graduate Student (0 vs. 1) into 0's
df$GraduateStudent<-ifelse(is.na(df$GraduateStudent)==T, 0, df$GraduateStudent)
# Create a new dataframe to display missing data for each variable
missing_df<-as.data.frame(matrix(data=NA, ncol = 3, nrow = ncol(df)))
colnames(missing_df)<-c("variable", "missingcount", "missingproportion")
missing_df$variable<-colnames(df)
for(i in 1:ncol(df)){
missing_df[i,2]<-sum(is.na(df[,i])==T)
missing_df[i,3]<-sum(is.na(df[,i])==T) / nrow(df)
}
# Compile list of variables that are missing 100% of rows...
missing_all<-missing_df[missing_df$missingproportion==1,]
allmissing_list<-missing_all$variable
# Print the list of variables that are missing at 100%
allmissing_list
# Make new dataframe that removes all variables that have 100% missingness
df_missing_removed<-dplyr::select(df, -allmissing_list)
colnames(df_missing_removed)
# Removing more columns that cannot be used (i.e., TEXT responses to some 'other' items)
df_missing_removed<- df_missing_removed[,-c(1:8, 10:20, 23:26, 15,16,26,36,52,53,56,58,68:69,71:80,90:95,115,121:287,324:334,349:410,420:424)]
finalfit::missing_plot(df_missing_removed)
colnames(df_missing_removed)
# Find column numbers for variables that need to be converted to "numeric"
# Also turns NAs in these columns into 0's
numcols<-c(1,3:28,32:39, 42:55, 68:70, 73:109)
for(col in numcols){
df_missing_removed[,col]<-as.numeric(unlist(df_missing_removed[,col]))
df_missing_removed[,col]<-ifelse(is.na(df_missing_removed[,col])==T, 0, df_missing_removed[,col])
}
finalfit::missing_plot(df_missing_removed)
# Create a new dataframe to display missing data for each variable
missing_df<-as.data.frame(matrix(data=NA, ncol = 3, nrow = ncol(df_missing_removed)))
colnames(missing_df)<-c("variable", "missingcount", "missingproportion")
missing_df$variable<-colnames(df_missing_removed)
for(i in 1:ncol(df_missing_removed)){
missing_df[i,2]<-sum(is.na(df_missing_removed[,i])==T)
missing_df[i,3]<-sum(is.na(df_missing_removed[,i])==T) / nrow(df_missing_removed)
}
# Remove last bits of high missingness in dataframe
colnames(df_missing_removed)
df_missing_removed<-dplyr::select(df_missing_removed, -B_Transgender, -S_LangChoice)
finalfit::missing_plot(df_missing_removed)
# Remove NA cases from final dataframe (clean_df)
clean_df<-na.omit(df_missing_removed)
finalfit::missing_plot(clean_df)
colnames(clean_df)
# Recorded Date and Student Stress Level ----------------------------------
# First 2 weeks of semester is very low stress - syllabi weeks, no quizzes and minimal homework
# Stress increases around the first month for most students probably
# Stress peaks for most student probably reading week before finals and before grades come back
clean_df$RecordedDate
# Critical Dates for stress in the semester:
# Lowest Stress (0) = 1/17/24 - 2/14/24
# Coasting Stress (1) = 2/15/24 - 3/15/24
# Lower Stress (0) = 3/16/24 - 3/22/24
# Highest Stress (1) = 3/23/24 - 5/8/24
## CHAT GPT PROMPT ##
# I have a column called RecordedDate in an R dataframe called clean_df. Each row of RecordedDate is formatted like this: 2/20/2024 10:51
# Can you write me some R code using the lubridate package that will create a new variable called SemesterStress
# Use the values of RecordedDate to assign SemesterStress a 0 or 1 according to the guidelines below:
# SemesterStress = 0 if RecordedDate is 1/17/24 - 2/14/24 or 3/16/24 - 3/22/24
# SemesterStress = 1 if RecordedDate is 2/15/24 - 3/15/24 or 3/23/24 - 5/8/24
# Load the necessary library
library(lubridate)
# Convert RecordedDate to datetime format
clean_df$RecordedDate <- mdy_hm(clean_df$RecordedDate)
# Create the SemesterStress variable
clean_df$SemesterStress <- with(clean_df, ifelse(
(RecordedDate >= mdy("1/17/2024") & RecordedDate <= mdy("2/14/2024")) |
(RecordedDate >= mdy("3/16/2024") & RecordedDate <= mdy("3/22/2024")),
0,
ifelse(
(RecordedDate >= mdy("2/15/2024") & RecordedDate <= mdy("3/15/2024")) |
(RecordedDate >= mdy("3/23/2024") & RecordedDate <= mdy("5/8/2024")),
1,
NA  # Assign NA if the date does not fall into any of the specified ranges
)
))
table(clean_df$SemesterStress)
hist(clean_df$SemesterStress)
# Display the dataframe with the new SemesterStress variable
print(clean_df)
head(clean_df)
# Prepare data for investigating depression
clean_df<-clean_df[,-c(2)]
colnames(clean_df)
clean_df<-clean_df[clean_df$Hispanic < 94, ]
clean_df<-clean_df[clean_df$UA_SUICIDE_Attempted < 5, ]
clean_df<-na.omit(clean_df)
finalfit::missing_plot(clean_df)
clean_df$SemesterStress<-as.numeric(unlist(clean_df$SemesterStress))
clean_df<-clean_df[clean_df$UA_SUICIDE_Considered < 5, ]
table(clean_df$UA_SUICIDE_Considered)
colnames(clean_df)
# Splitting the Data into Training and Testing subsamples -----------------
library(caTools)
set.seed(123)  # For reproducibility
split <- sample.split(clean_df$UA_SUICIDE_Attempted, SplitRatio = 0.7)  # 70% training data
# Create training and testing sets
training <- subset(clean_df, split == TRUE)
testing <- subset(clean_df, split == FALSE)
# Separating all predictors (X) from outcome (y) --------------------------
X_train<-training[,-c(120)]
X_test<-testing[,-c(120)]
y_train<-training[,c(120)]
y_test<-testing[,c(120)]
colnames(X_train)
# Load the reticulate package & Activate python environment ---------------
library(reticulate)
use_condaenv("mlbase", required=T)
py_config()
# Convert R objects to PY objects (X_train_py is the python object --------
X_train_py<-r_to_py(X_train)
X_test_py<-r_to_py(X_test)
y_train_py<-r_to_py(y_train)
y_test_py<-r_to_py(y_test)
reticulate::repl_python()
# Grid Search CV
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import r2_score
from sklearn.pipeline import Pipeline
# Read Data
X_train = r.X_train_py
X_test = r.X_test_py
y_train = r.y_train_py
y_test = r.y_test_py
# The KFold object in SciKit Learn tells the cross validation object (see below) how to split up the data:
# Initiate KFold Object
kf = KFold(shuffle=True, random_state=777777, n_splits=4)
kf.split(X_train) # is a generator object
X_train.describe()
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import GridSearchCV
# Same estimator as before:
estimator = Pipeline([("scaler", StandardScaler()),
("ridge_regression", Ridge())])
params = {
'ridge_regression__alpha': np.geomspace(4, 20, 30),
}
grid = GridSearchCV(estimator, params, cv=kf)
grid.fit(X_train, y_train)
grid.best_score_, grid.best_params_
y_predict_test = grid.predict(X_test)
y_predict_train = grid.predict(X_train)
r2_score(y_train, y_predict_train)
# This includes both in-sample and out-of-sample
r2_score(y_test, y_predict_test)
# Notice that "grid" is a fit object!
# We can use grid.predict(X_test) to get brand new predictions!
Ridge_Coefs = grid.best_estimator_.named_steps['ridge_regression'].coef_
Varnames = X_train.columns
# Zip the two lists together
zipped_list = list(zip(Varnames, Ridge_Coefs))
# Convert the zipped list to a DataFrame
df = pd.DataFrame(zipped_list, columns=['Variable', 'Coefficient'])
# Print the DataFrame
df.sort_values(by = 'Coefficient', ascending = False)
print(df)
df.to_csv('Ridge_Coefficients.csv')
GridResults = pd.DataFrame(grid.cv_results_)
GridResults.rank_test_score.sort_values()
print(GridResults.loc[29])
print(GridResults.params.loc[29])
grid.best_estimator_
quit
library(caret)
library(caTools)
ridge_coef<- read.csv("Ridge_Coefficients.csv", header = T, sep = ',', encoding = 'utf-8-rom')
ridge_coef$Abs_value <- abs(ridge_coef$Coefficient)
ridge_coef<- ridge_coef[ridge_coef$Abs_value > .03, ]
New_df<-dplyr::select(clean_df, UA_SUICIDE_Attempted, ridge_coef$Variable)
set.seed(123)  # For reproducibility
split <- sample.split(New_df$UA_SUICIDE_Attempted, SplitRatio = 0.7)  # 70% training data
New_df<-na.omit(New_df)
for(i in 1:11){
New_df[,i]<-mean(New_df[,i])-New_df[,i]
}
New_df$UA_SUICIDE_Attempted
ridge_coef<- read.csv("Ridge_Coefficients.csv", header = T, sep = ',', encoding = 'utf-8-rom')
ridge_coef$Abs_value <- abs(ridge_coef$Coefficient)
ridge_coef<- ridge_coef[ridge_coef$Abs_value > .03, ]
New_df<-dplyr::select(clean_df, UA_SUICIDE_Attempted, ridge_coef$Variable)
set.seed(123)  # For reproducibility
split <- sample.split(New_df$UA_SUICIDE_Attempted, SplitRatio = 0.7)  # 70% training data
New_df<-na.omit(New_df)
for(i in 2:19){
New_df[,i]<-mean(New_df[,i])-New_df[,i]
}
New_df$UA_SUICIDE_Attempted
# Create training and testing sets
training <- subset(New_df, split == TRUE)
testing <- subset(New_df, split == FALSE)
fit<-lm(UA_SUICIDE_Attempted~., data = training)
summary(fit)
# Generate predictions
pred <- round(predict(fit, testing), 0)
# Calculate error
error <- pred - testing$UA_SUICIDE_Attempted
# Calculate MAE
MAE <- mean(abs(error))
print(paste("Mean Absolute Error (MAE):", round(MAE, 3)))
# Calculate Standard Error
SE <- sd(error)
print(paste("Standard Error (SE):", round(SE, 3)))
# Plot histogram of errors
hist(error, main="Histogram of Prediction Errors", xlab="Error")
# Calculate mean of the actual values
mean_actual <- mean(testing$UA_SUICIDE_Attempted)
print(paste("Mean of Actual Values:", round(mean_actual, 3)))
# Calculate Total Sum of Squares (TSS)
TSS <- sum((testing$UA_SUICIDE_Attempted - mean_actual)^2)
# Calculate Residual Sum of Squares (RSS)
RSS <- sum(error^2)
# Calculate R-squared (R2)
R2 <- 1 - (RSS/TSS)
print(paste("R-squared (R2):", round(R2, 3)))
pred
